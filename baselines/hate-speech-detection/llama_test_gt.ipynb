{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!git clone https://github.com/Vicomtech/hate-speech-dataset.git\n",
    "\n",
    "dataset_path = \"hate-speech-dataset/sampled_test\" \n",
    "annotations_path = \"hate-speech-dataset/annotations_metadata.csv\" \n",
    "annotations_df = pd.read_csv(annotations_path)\n",
    "annotations_df = annotations_df[[\"file_id\", \"label\"]]\n",
    "label_map = {\"noHate\": 0, \"hate\": 1}\n",
    "annotations_df[\"label\"] = annotations_df[\"label\"].map(label_map)\n",
    "label_dict = dict(zip(annotations_df[\"file_id\"], annotations_df[\"label\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_text_files(folder_path, label_dict):\n",
    "    dataset = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_id = filename.replace(\".txt\", \"\")  \n",
    "            \n",
    "            if file_id in label_dict:\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    text = file.read().strip()\n",
    "                    dataset.append({\"text\": text, \"label\": label_dict[file_id]})\n",
    "    return dataset\n",
    "\n",
    "processed_ds = load_text_files(dataset_path, label_dict)\n",
    "print(f\"Loaded {len(processed_ds)} samples from {dataset_path}\")\n",
    "print(\"Sample data:\", processed_ds[:5])\n",
    "\n",
    "def extract_yes_no(response):\n",
    "    response = response.lower().strip()\n",
    "    if \"yes\" in response.split() or \"it is fair\" in response:\n",
    "        return 0  # fair -> not hate speech\n",
    "    elif \"no\" in response.split() or \"it is unfair\" in response or \"it is not fair\" in response:\n",
    "        return 1  # unfair -> hate speech\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def think_twice(claim, model, tokenizer):\n",
    "    prompt_suspicious = f\"Find the most suspicious part of the statement:\\n\\n{claim}\\n\\nSuspicious Part:\"\n",
    "    inputs = tokenizer(prompt_suspicious, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "    suspicious_info = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    suspicious_info = suspicious_info.replace(prompt_suspicious, \"\").strip()\n",
    "    print(\"\\nðŸ”¹ Suspicious Info:\", suspicious_info)\n",
    "    \n",
    "    prompt_fact = f\"Given the statement:\\n\\n{claim}\\n\\nThe most suspicious part is: {suspicious_info}\\n\\nProvide a relevant social or scientific fact to verify it:\\nFact:\"\n",
    "    inputs = tokenizer(prompt_fact, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=30)\n",
    "    fact = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    fact = fact.replace(prompt_fact, \"\").strip()\n",
    "    print(\"ðŸ”¹ Generated Fact:\", fact)\n",
    "    \n",
    "    final_prompt = f\"Claim: {claim}\\nThe claim mentions that {suspicious_info}.\\nFact: {fact}\\nQuestion: Is it fair to say that?\\nAnswer:\"\n",
    "    inputs = tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=5)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.replace(final_prompt, \"\").strip()\n",
    "    print(\"ðŸ”¹ Final Answer:\", response)\n",
    "    return extract_yes_no(response)  # yes -> 0 (fair), no -> 1 (unfair)\n",
    "\n",
    "HUGGINGFACE_TOKEN = \"\"  # Replace with your token\n",
    "\n",
    "login(token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Loading LLaMA-2-7b model...\")\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "demo_samples = processed_ds[:5]\n",
    "\n",
    "print(\"\\nProcessing demo samples:\")\n",
    "for i, sample in enumerate(demo_samples, 1):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"Text: {sample['text']}\")\n",
    "    print(f\"Ground Truth: {'Unfair (Hate)' if sample['label'] == 1 else 'Fair (Not Hate)'}\")\n",
    "    \n",
    "    # Process with think_twice\n",
    "    prediction = think_twice(sample['text'], model, tokenizer)\n",
    "    print(f\"Prediction: {'Unfair (Hate)' if prediction == 1 else 'Fair (Not Hate)'}\")\n",
    "\n",
    "print(\"\\nProcessing all samples for evaluation...\")\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "for sample in tqdm(processed_ds):\n",
    "    try:\n",
    "        prediction = think_twice(sample['text'], model, tokenizer)\n",
    "        predictions.append(prediction)\n",
    "        ground_truth.append(int(sample['label']))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample: {e}\")\n",
    "        predictions.append(1)\n",
    "        ground_truth.append(int(sample['label']))\n",
    "\n",
    "accuracy = accuracy_score(ground_truth, predictions)\n",
    "f1 = f1_score(ground_truth, predictions, average='weighted')\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(ground_truth, predictions, target_names=[\"Fair (Not Hate)\", \"Unfair (Hate)\"]))\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'text': [sample['text'] for sample in processed_ds],\n",
    "    'ground_truth': ground_truth,\n",
    "    'prediction': predictions,\n",
    "    'correct': [gt == pred for gt, pred in zip(ground_truth, predictions)]\n",
    "})\n",
    "\n",
    "results_df.to_csv('hate_speech_results.csv', index=False)\n",
    "print(\"\\nResults saved to hate_speech_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
